{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dafec96d-031d-43d8-a400-772951d82599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50V2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from tf_keras_vis.gradcam import Gradcam\n",
    "from tf_keras_vis.utils import normalize\n",
    "#from keras import backend as K\n",
    "#from hyperopt import hp, fmin, tpe, Trials\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "from keras.models import load_model\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdc9b44d-b1cc-4b5d-b78c-bac2b5a6139c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8187ef57-df25-4116-b443-1fd9870bc756",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "from skimage import exposure\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c560d4a-6c9f-46dc-91d5-c1cf3f629c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "import os\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59a95490-1507-4b08-a65a-4577951f41b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dc15632-292a-40df-a894-1f1b96bbe595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU device found\n"
     ]
    }
   ],
   "source": [
    "if tf.test.gpu_device_name():\n",
    "    print(\"Default GPU device:\", tf.test.gpu_device_name())\n",
    "else:\n",
    "    print(\"No GPU device found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dec10cb6-27f5-408c-8796-52a9cd58789c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"chexnet_train.csv\")\n",
    "val_df = pd.read_csv(\"chexnet_val.csv\")\n",
    "test_df = pd.read_csv(\"chexnet_test.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3566f024-9876-4746-a868-58928fd75824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train_df, inter_df = train_test_split(df, test_size=0.2, random_state=42)\\nval_df, test_df = train_test_split(inter_df, test_size=0.4, random_state=42)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, inter_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(inter_df, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fd11a34-c6fd-47ee-b776-a5ab37c3c972",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 14\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7c76897-6ede-41c7-abd2-bfbd65d5043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_df.columns[1:].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3a3fcd0-663c-42c2-8fa7-611f684e493e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Effusion', 'Emphysema', 'Fibrosis', 'Infiltration', 'Mass', 'Nodule', 'Pleural_Thickening', 'Pneumonia', 'Pneumothorax', 'No Finding']\n"
     ]
    }
   ],
   "source": [
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "071b0445-267f-4be9-a120-044c3da35255",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Cardiomegaly',\n",
    "          'Emphysema', \n",
    "          'Effusion', \n",
    "          'Infiltration', \n",
    "          'Mass', \n",
    "          'Nodule', \n",
    "          'Atelectasis',\n",
    "          'Pneumothorax',\n",
    "          'Pleural_Thickening', \n",
    "          'Pneumonia', \n",
    "          'Fibrosis', \n",
    "          'Edema', \n",
    "          'Consolidation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a90cb373-8f97-4a09-b3ce-8aa5d3ba5f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leakage between train and test: False\n",
      "leakage between valid and test: False\n"
     ]
    }
   ],
   "source": [
    "def check_for_leakage(df1, df2, patient_col):\n",
    "    \n",
    "    df1_patients_unique = set(df1[patient_col].values)\n",
    "    df2_patients_unique = set(df2[patient_col].values)\n",
    "    \n",
    "    patients_in_both_groups = df1_patients_unique.intersection(df2_patients_unique)\n",
    "\n",
    "    leakage = len(patients_in_both_groups) > 0\n",
    "    \n",
    "    return leakage\n",
    "print(\"leakage between train and test: {}\".format(check_for_leakage(train_df, test_df, 'id')))\n",
    "print(\"leakage between valid and test: {}\".format(check_for_leakage(val_df, test_df, 'id')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c6e1e57-a44b-4a3b-8ee5-5b0aadfc14aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score_normalization(image):\n",
    "    mean = np.mean(image)\n",
    "    std = np.std(image)\n",
    "    return (image - mean) / (std + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2aecb734-2681-46cb-a7e7-061a7169356b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_grayscale(img_array):\n",
    "    r, g, b = img_array[:,:,0], img_array[:,:,1], img_array[:,:,2]\n",
    "    return 0.2126 * r + 0.7152 * g + 0.0722 * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a13a3d8d-95ad-4bde-b809-b3a5d37db6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    image = exposure.equalize_adapthist(image/np.max(image))\n",
    "    #image = exposure.equalize_hist(image)\n",
    "    image = image / 255.0\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e5c384-0437-4e34-bc15-81c320d03409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import exposure\n",
    "\n",
    "# Load the train.csv file\n",
    "train_df = pd.read_csv('Chexnet_train.csv')\n",
    "\n",
    "# Specify the path to the image directory\n",
    "image_dir = r\"Images/\"\n",
    "\n",
    "# Iterate over the images in the train.csv file\n",
    "for index, row in train_df.iterrows():\n",
    "    # Load the image\n",
    "    image_path = os.path.join(image_dir, row['id'])\n",
    "    image = cv2.imread(image_path)\n",
    "    image = image.astype(np.float32) / 255.0\n",
    "    image = np.array(image)\n",
    "    \n",
    "    # Apply adaptive histogram equalization\n",
    "    equalized_image = exposure.equalize_adapthist(image)\n",
    "    \n",
    "    # Display the original and equalized images\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    plt.imshow(image)\n",
    "    plt.title('Original Image')\n",
    "    plt.imshow(equalized_image)\n",
    "    plt.title('Equalized Image')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "feb462c2-18e7-4f9d-a149-03104fe991ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28430 validated image filenames.\n",
      "Found 226 validated image filenames.\n",
      "Found 450 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    preprocessing_function = preprocess_image,\n",
    "    zoom_range=0.2, \n",
    "    shear_range=0.2,\n",
    "    horizontal_flip=False)\n",
    "\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "    dataframe= train_df,\n",
    "    directory= \"Images/\",\n",
    "    x_col='id',\n",
    "    y_col=class_names,\n",
    "    target_size=(224, 224),\n",
    "     class_mode=\"raw\"\n",
    "  \n",
    ")\n",
    "\n",
    "val_generator = datagen.flow_from_dataframe(\n",
    "    dataframe= val_df,\n",
    "    directory= \"Images/\",\n",
    "    x_col='id',\n",
    "    y_col=class_names,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "     class_mode=\"raw\"\n",
    "  \n",
    ")\n",
    "\n",
    "test_generator = datagen.flow_from_dataframe(\n",
    "    dataframe= test_df,\n",
    "    directory= \"Images/\",\n",
    "    x_col='id',\n",
    "    y_col=class_names,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode = \"raw\", \n",
    "    shuffle=False )\n",
    "\n",
    "\n",
    "base_model = ResNet50V2(weights='imagenet', include_top= False, input_shape=(224, 224, 3))\n",
    "\n",
    "for layer in base_model.layers[:-2]:  \n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='gelu', kernel_regularizer=l2(0.005))(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(256, activation='gelu', kernel_regularizer=l2(0.005))(x)\n",
    "x = Dropout(0.2)(x)\n",
    "predictions = Dense(14, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', 'precision', 'recall'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37ecd4d-104b-4f8c-984b-2d2d93435fbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd95f561-8d19-403c-9d6a-1c0dd1a8364f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_df.columns[1:]  # extract class names from column headers (excluding the first column)\n",
    "selected_columns = [label for label in labels if label in train_df.columns]\n",
    "class_labels = train_df[selected_columns].values.argmax(axis=1)  # get the class labels from the one-hot encoded columns\n",
    "\n",
    "class_weights = sklearn.utils.compute_class_weight(class_weight='balanced', \n",
    "                                                  classes=np.unique(class_labels), \n",
    "                                                  y=class_labels)\n",
    "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03faf9a-5e86-465a-941f-eea721188bba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680ebdfd-d715-40c6-865d-91d41fa48ce2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',  \n",
    "    factor=0.3,  \n",
    "    patience=2,  \n",
    "    min_lr=0.0005 \n",
    ")\n",
    "    \n",
    "#early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "  \n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=6,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[reduce_lr]\n",
    "    \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b12401-abcd-424b-9f2e-202c851ae1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('chexres50.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d50316-e1e0-4b01-8939-32e487fae644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(test_generator)\n",
    "print(f'Test accuracy: {accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64626a2-7fb7-48a1-9025-af08b5fc6f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_vals = model.predict(test_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ce0b8b-972c-45d1-91fa-3bc6f89fbef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Cardiomegaly', \n",
    "          'Emphysema', \n",
    "          'Effusion', \n",
    "          'Hernia', \n",
    "          'Infiltration', \n",
    "          'Mass', \n",
    "          'Nodule', \n",
    "          'Atelectasis',\n",
    "          'Pneumothorax',\n",
    "          'Pleural_Thickening', \n",
    "          'Pneumonia', \n",
    "          'Fibrosis', \n",
    "          'Edema', \n",
    "          'Consolidation',\n",
    "           'No Finding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7b74a3-6e44-4f44-becc-b9b0e8ad1d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roc_curve(labels, predicted_vals, generator):\n",
    "    auc_roc_vals = []\n",
    "    for i in range(len(labels)):\n",
    "        try:\n",
    "            gt = generator.labels[:, i]\n",
    "            pred = predicted_vals[:, i]\n",
    "            auc_roc = roc_auc_score(gt, pred)\n",
    "            auc_roc_vals.append(auc_roc)\n",
    "            fpr_rf, tpr_rf, _ = roc_curve(gt, pred)\n",
    "            plt.figure(1, figsize=(10, 10))\n",
    "            plt.plot([0, 1], [0, 1], 'k--')\n",
    "            plt.plot(fpr_rf, tpr_rf,\n",
    "                     label=labels[i] + \" (\" + str(round(auc_roc, 3)) + \")\")\n",
    "            plt.xlabel('False positive rate')\n",
    "            plt.ylabel('True positive rate')\n",
    "            plt.title('ROC curve')\n",
    "            plt.legend(loc='best')\n",
    "        except:\n",
    "            print(\n",
    "                f\"Error in generating ROC curve for {labels[i]}. \"\n",
    "                f\"Dataset lacks enough examples.\"\n",
    "            )\n",
    "    plt.show()\n",
    "    return auc_roc_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cccd92-ced8-4b7d-9f73-a6e43a12e4c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predicted_vals = model.predict(test_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cc6aae-d986-4ec8-a908-141491a08a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_rocs = util.get_roc_curve(labels, predicted_vals, test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd69d84-2266-4b5b-a3a7-8b6832313d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dbec20-48c6-4b8c-83e5-c463fb375bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.densenet import preprocess_input, decode_predictions\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = image.load_img(image_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    return tf.keras.applications.densenet.preprocess_input(img_array)\n",
    "\n",
    "\n",
    "\n",
    "def get_gradcam(model, img, layer_name):\n",
    "    \n",
    "    img_array = img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = preprocess_input(img_array)\n",
    "    \n",
    "    grad_model = Model(inputs=model.inputs, outputs=[model.get_layer(layer_name).output, model.output])\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = grad_model(img_array)\n",
    "        class_idx = tf.argmax(predictions[0])\n",
    "\n",
    "    output = conv_outputs[0]\n",
    "    grads = tape.gradient(predictions, conv_outputs)[0]\n",
    "    guided_grads = tf.cast(output > 0, 'float32') * tf.cast(grads > 0, 'float32') * grads\n",
    "\n",
    "    weights = tf.reduce_mean(guided_grads, axis=(0, 1))\n",
    "    cam = tf.reduce_sum(tf.multiply(weights, output), axis=-1)\n",
    "    heatmap = np.maximum(cam, 0)\n",
    "    heatmap /= tf.reduce_max(heatmap)\n",
    "    heatmap_img = plt.cm.jet(heatmap)[..., :3]\n",
    "\n",
    "    # Load the original image\n",
    "    original_img = Image.fromarray(img)\n",
    "\n",
    "    # Resize the heatmap to match the original image size\n",
    "    heatmap_img = Image.fromarray((heatmap_img * 255).astype(np.uint8))\n",
    "    heatmap_img = heatmap_img.resize(original_img.size)\n",
    "\n",
    "    # Overlay the heatmap on the original image\n",
    "    overlay_img = Image.blend(original_img, heatmap_img, 0.5)\n",
    "\n",
    "    # Return the overlayed image\n",
    "    return overlay_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f7587e-7ebc-4938-a3bb-adceaec83ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_decode_predictions(predictions, class_labels):\n",
    "    \n",
    "    decoded_predictions = []\n",
    "    for pred in predictions:\n",
    "        # Get indices of top predicted classes\n",
    "        top_indices = pred.argsort()[-3:][::-1]  # Change 5 to the number of top classes you want to retrieve\n",
    "        # Decode each top predicted class\n",
    "        decoded_pred = [(class_labels[i], pred[i]) for i in top_indices]\n",
    "        decoded_predictions.append(decoded_pred)\n",
    "    return decoded_predictions\n",
    "\n",
    "def classify_image(img):\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = preprocess_input(img_array)\n",
    "\n",
    "\n",
    "    predictions1 = model.predict(img_array)\n",
    "    decoded_predictions = custom_decode_predictions(predictions1, class_names)\n",
    "    overlay_img = get_gradcam(model, img, layer_name)\n",
    "\n",
    "    # Return the decoded predictions and the overlayed image\n",
    "    return decoded_predictions, overlay_img\n",
    "# Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=classify_image, \n",
    "    inputs=\"image\", \n",
    "    outputs=[\"text\", \"image\"],  # Add an \"image\" output for the overlayed image\n",
    "    title=\"Xray Classification - KIMS\",\n",
    "    description=\"Classify cxr into one of 20 classes - Atelectasis, Cardiomegaly, Consolidation, Edema, Effusion, Emphysema, Fibrosis, Hernia, Infiltration, Mass, Nodule, Pleural Thickening, Pneumonia, Pneumothorax, Pneumoperitoneum, Pneumomediastinum, Subcutaneous Emphysema, Tortuous Aorta, Calcification of the Aorta, No Finding. Built by Dr Sai and Dr Ajavindu\"\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "iface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
